Problem Statement:
Let’s design a Web Crawler document on how to crawl a website site and save the images and text from the website.

step-1:
Web Crawler -

A web crawler is a software program which browses the World Wide Web in a methodical and automated manner.
It collects documents by recursively fetching links from a set of starting pages.
Many sites, particularly search engines, use web crawling as a means of providing up-to-date data.
Search engines download all the pages to create an index on them to perform faster searches.
A web crawler works by discovering URLs and reviewing and categorizing web pages. 
Along the way, they find hyperlinks to other webpages and add them to the list of pages to crawl next. 
Web crawlers are smart and can determine the importance of each web page.

step-2:
Requirements and Goals -

Scalability: Our service needs to be scalable such that it can crawl the entire Web, and can be used to fetch hundreds of millions of Web documents.
Extensibility: Our service should be designed in a modular way, with the expectation that new functionality will be added to it. There could be newer document types that needs to be downloaded and processed in the future.

step-3:
High Level Design-

Basic Algorithm-
The basic algorithm executed by any Web crawler is to take a list of seed URLs as its input.
And then repeatedly execute the following steps:
Pick a URL from the unvisited URL list.
Determine the IP Address of its host-name.
Establishing a connection to the host to download the corresponding document.
Parse the document contents to look for new URLs.
Add the new URLs to the list of unvisited URLs.
Process the downloaded document, e.g.,store it or index its contents, etc.
Go back to step-1.

How To Crawl -
Breadth First or Depth First 

Breadth-first search (BFS) is usually used.
However, Depth First Search (DFS) is also utilized in some situations, like if crawler has already established a connection with the website, it might just DFS all the URLs within this website to save some handshaking overhead.

Path-ascending crawling:

Path-ascending crawling can help discover a lot of isolated resources or resources for which no inbound link would have been found in regular crawling of a particular Web site.
In this scheme, a crawler would ascend to every path in each URL that it intends to crawl.
For example, when given a seed URL of http://foo.com/a/b/page.html, it will attempt to crawl /a/b/, /a/, and /.


step-4:
 how to save images and text from website -
 
Right click the URL, choose Open in a new tab.
In the new tab, right click the image and Save image as to then give the image a name and choose the destination on your pc to save it to.

Click the “File” menu in your Web browser and click the “Save as” or “Save Page As” option. Select “Web Page, Complete” from the Save as Type drop-down menu and type a name for the file. Click “Save.” The text and images from the Web page will be extracted and saved.
